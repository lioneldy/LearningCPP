## 瓮球模型

假设一个房间内有N个瓮，每个瓮里面有大量彩色的球，假设总共有M种颜色的球，随机选择一个瓮作为初始瓮，然后从中随机拿取一个球，记录球的颜色并放回。然后随机重新选择一个瓮，从中随机拿一个球记录颜色并放回，不断重复T次，得到一组观测序列$$O=\{绿，绿，蓝，红，黄，红,\cdots,蓝\}$$

|$瓮_1$|$瓮_2$|$\cdots$|$瓮_N$|
|:----:|:----:|:---:|:----:|
|$P(红)=b_1(1)$|$P(红)=b_2(1)$|$\cdots$|$P(红)=b_N(1)$|
|$P(蓝)=b_1(2)$|$P(蓝)=b_2(2)$|$\cdots$|$P(蓝)=b_N(2)$|
|$P(绿)=b_1(3)$|$P(绿)=b_2(3)$|$\cdots$|$P(绿)=b_N(3)$|
|$\vdots$|$\vdots$||$\vdots$|
|$P(黄)=b_1(M)$|$P(黄)=b_2(M)$|$\cdots$|$P(黄)=b_N(M)$|

其中$b_i(M)$表示从第i个瓮中取出第M种颜色的概率。整个过程中，因为只知道最后观测到的颜色序列，并不知道每种颜色是从哪个瓮里面取出来的，所以叫隐式马尔科夫过程，选取的瓮称为**隐式状态**，而观测到的序列称为**隐式状态的表现**。

## 隐式马尔科夫模型（HMM）的三个基本问题

1. 概率计算问题
	- 给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1,o_2,\cdots,o_T)$ ，计算在模型 $\lambda$ 下观测序列$O$出现的概率 $P(O\mid\lambda)$ 。
2. 预测问题
	- 也称为解码（decoding）问题。已知模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1,o_2,\cdots,o_T)$ ，求对给定观测序列条件概率 $P(I\mid O)$ 最大的状态序列 $I=(i_1,i_2,\cdots,i_T)$ 。即给定观测序列，求最有可能的对应的状态序列。
3. 学习问题
	- 已知观测序列 $O=(o_1,o_2,\cdots,o_T)$ ，估计模型 $\lambda=(A,B,\pi)$ 参数，使得在该模型下观测序列概率 $P(O\mid\lambda)$ 最大。即使用极大似然估计的方法估计参数。

#### 问题1的解法
------
为了解决问题1，如果采用直接计算法，利用概率公式直接计算:

$$P(O\mid\lambda)=\sum_{i_1,i_2,\cdots,i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}\pi_{i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_{T-2}}b_{i_T}(o_T)$$

其中，初始状态是i的概率是$\pi_i$，在第N种状态下出现$o_i$的概率是$b_{i_N}(o_1)$，状态i到状态j的转移概率为$a_{ij}$。可以这么理解，观测序列有T个观测值，每个观测值都可能是N中状态下出现的，所以需要分别计算$N^T$种隐式状态链下满足观测序列的概率并求和，这样一来上面这个式子的计算量就很大了，是$O(TN^T)$阶的。

**前向算法（forward algorithm）**的目的是优化上述算法，给定隐式马尔科夫模型$\lambda$，定义到时刻t时的部分观测序列为$o_1,o_2,\cdots,o_t$，且时刻t的状态为$q_i$的概率为前向概率，记作$$\alpha_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i\mid\lambda)$$

##### 前向算法实现：

输入：隐式马尔科夫模型$\lambda$，观测序列$O$；

输出：观测序列概率$P(O\mid\lambda)$。

1. 初值$$\alpha_1(i)=\pi_ib_i(o_1)\qquad i=1,2\cdots,N$$表示初始状态为i且观测值为$o_1$的概率
2. 递推$\quad$ 对$t=1,2,\cdots,T-1$，$$\alpha_{t+1}(i)=\left[\sum_{j=1}^N\alpha_t(j)a_{ji}\right]b_i(o_{t+1})\qquad i=1,2\cdots,N$$这个式子意味着$t+1$时刻，状态为$q_i$且观测值为$o_{t+1}$的前向概率由$t$时刻$N$个状态的前向概率乘以各个状态转移到状态$i$的转移概率$a_{ji}$之和再乘以状态$i$时观测值为$o_{t+1}$的概率$b_i(o_{t+1})$而得到
3. 终止$$P(O\mid\lambda)=\sum_{i=1}^N\alpha_T(i)$$即观测序列为$O$的概率为$T$（最终）时刻状态为$i=1,2,\cdots,N$的前向概率之和

这样一来，计算初始前向概率需要计算$N$次，每多递推一个观测值需要计算$N^2$次，递推出整个观测序列的概率的计算量就降到了$O(N^2T)$阶。实际上解决问题一前向算法就够了，后向算法是为了接下来的其他计算更为方便。

**后向算法（backward algorithm）** 给定隐式马尔科夫模型$\lambda$，定义时刻t时状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列为$o_{t+1},o_{t+2},\cdots,o_T$的概率为后向概率，记作$$\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_t\mid i_t=q_i,\lambda)$$

##### 后向算法实现

输入：隐式马尔科夫模型$\lambda$，观测序列$O$；

输出：观测序列概率$P(O\mid\lambda)$。

1. 初值$$\beta_T(i)=1\qquad i=1,2\cdots,N$$初始化后向概率，对最终时刻的所有状态$q_i$规定$\beta_T(i)=1$
2. 递推$\quad$ 对$t=T-1,T-2,\cdots,1$，$$\beta_t(i)=\sum_{j=1}^Na_{ij}b_i(o_{t+1})\beta_t(j)\qquad i=1,2\cdots,N$$这个式子意味着$t$时刻，状态为$q_i$且$t+1$之后的观测序列为$o_{t+1},o_{t+2},\cdots,o_T$的后向概率，由$t+1$时刻$N$个状态的后向概率乘以各个状态转移到状态$j$的转移概率$a_{ij}$再乘以状态$i$时观测值为$o_{t+1}$的概率$b_i(o_{t+1})$之和而得到
3. 终止$$P(O\mid\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)$$思路同步骤2，只是初始概率$\pi_i$代替了转移概率

由前向-后向概率的定义可以化简观测序列概率$$P(O\mid\lambda)=\sum_{i=1}^N\alpha_t(i)\beta_t(i)$$

#### 问题2的解法
----
为了实现问题2的解法，我们定义变量$$\gamma_t(i)=P(i_t=q_i\mid O,\lambda)$$表示给定模型$\lambda$和观测序列$O$，在时刻$t$处于状态$q_i$的概率，利用前向-后向概率的定义可以将其化简$$\gamma_t(i)=\frac {\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}$$利用$\gamma_t(i)$我们可以算出时刻$t$最有可能的隐式状态$$q_t=arg\max_{1\le i\le N}[\gamma_t(i)]\qquad 1\le t\le T$$从而得到一个最有可能的状态序列$Q=\{q_1,q_2,\cdots,q_t\}$，但是这样得到的状态序列中有可能存在转移概率为0的相邻状态，即对于某些$i,j,a_{ij}=0$时。为了解决这个问题，我们将采用基于动态规划的**维特比算法(Viterbi Algorithm)**。

根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻$t$通过节点$q_t^*$，那么这一路径从节点$q_t^*$到终点$q_T^*$的部分路径，对于从$q_t^*$到$q_T^*$的所有可能的部分路径来说，必须是最优的。因为假如不是这样，那么从$q_t^*$到$q_T^*$就有另外一条更好的路径存在，如果把它和从$q_1^*$到$q_t^*$的部分路径连接起来，就会一个比原来路径更优的路径，这与已知相矛盾。

根据这一特性，我们只需要从时刻$t=1$开始，递推地计算在时刻$t$状态为$q_i$的各条部分路径的最大概率，直至得到时刻$t=T$状态为$q_i$的各条路径的最大概率。时刻$t=T$时的最大概率即为最优路径的概率$P^*$，最优路径的终节点$q_T^*$也同时得到。之后，为了找出最优路径的各个节点，从终节点开始，由后向前逐步求得节点$q_{T-1}^*,q_{T-2}^*,\cdots,q_1^*$，得到最优路径。

##### 首先引入两个变量$\delta$和$\psi$。

定义在时刻$t$状态为$q_i$的所有单个路径$q_1,q_2,\cdots,q_t$中概率最大值为$$\delta_t(i)=\max_{q_1,q_2,\cdots,q_{t-1}}P(q_1,q_2,\cdots,q_t=i,o_1,o_2,\cdots,o_t\mid\lambda)$$由定义可得变量$\delta$的递推公式为$$\delta_{t+1}(j)=\left[\max_i\delta_t(i)a_{ij}\right]\cdot b_j(o_{t+1})\qquad t=1,2,\cdots,T-1$$

定义在时刻$t$状态为$q_i$的所有单个路径$q_1,q_2,\cdots,q_{t-1}，q_i$中概率最大的路径的第$t-1$个节点为$$\psi_t(i)=arg\max_{1\le j\le N}\left[\delta_{t-1}(j)a_{ji}\right]\qquad t=2,3,\cdots,T$$

##### 维特比算法实现

输入：隐式马尔科夫模型$\lambda$，观测序列$O$；

输出：最优路径$Q=\{q_1,q_2,\cdots,q_T\}$。

1. 初始化$$\delta_1(i)=\pi_ib_i(o_1)\qquad i=1,2,\cdots,N$$$$\psi_1(i)=0\qquad i=1,2,\cdots,N$$ 
2. 递推$$\delta_{t}(j)=\left[\max_{1\le i\le N}\delta_{t-1}(i)a_{ij}\right]\cdot b_j(o_t)\qquad t=2,3	,\cdots,T$$$$\psi_t(i)=arg\max_{1\le j\le N}\left[\delta_{t-1}(j)a_{ji}\right]\qquad t=2,3,\cdots,T$$
3. 终止$$P^*=\max_{1\le i\le N}[\delta_T(i)]$$$$q_T^*=arg\max_{1\le i\le n}[\delta_T(i)]$$
4. 路径回溯$$q_t^*=\psi_{t+1}\left(q_{t+1}^*\right)\qquad t=T-1,T-2,\cdots,1$$

这样就可以得到最优路径，问题2由此得到解决。

#### 问题3的解法
----
为了描述迭代更新参数模型的过程，我们引入变量$\xi_t(i,j)$表示在已给参数变量$\lambda$和观测序列$O$的情况下，$t$时刻状态为$S_i$且$t+1$时刻状态为$S_j$的概率，即$$\xi_t(i,j)=P(q_t=S_i,q_{t+1}=S_j\mid O,\lambda)$$在之前我们已经给过$\gamma_t(i)$的定义，由定义我们可知二者有以下关系$$\gamma_t(i)=\sum_{j=1}^N\xi_t(i,j)$$

##### Baum-Welch算法实现

输入：观测数据$O=(o_1,o_2,\cdots,o_t)$；

输出：隐式马尔科夫模型参数$\lambda(A,B,\pi)$。

1. 初始化
	- 对$n=0$，选取$a_{ij}^{(0)},b_j(k)^{(0)},\pi_i^{(0)}$，得到模型$\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$ 
2. 递推. 对$n=1,2,\cdots,$$$a_{ij}^{(n+1)}=\frac {\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}$$$$b_{ij}(k)^{(n+1)}=\frac {\sum_{t=1,o_t=v_k}^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(i)}$$$$\pi_i^{(n+1)}=\gamma_1(i)$$
3. 终止. 得到模型参数$\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})$
